{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPspEvoi/b9tH7/lI9NirDy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MeghanaShanthappa/TensorRT_features/blob/main/Autocast_gradscaler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "X6z0P-8HvlDl"
      },
      "outputs": [],
      "source": [
        "import torch, time, gc\n",
        "\n",
        "# Timing utilities\n",
        "start_time = None\n",
        "\n",
        "def start_timer():\n",
        "    global start_time\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.reset_max_memory_allocated()\n",
        "    torch.cuda.synchronize()\n",
        "    start_time = time.time()\n",
        "\n",
        "def end_timer_and_print(local_msg):\n",
        "    torch.cuda.synchronize()\n",
        "    end_time = time.time()\n",
        "    print(\"\\n\" + local_msg)\n",
        "    print(\"Total execution time = {:.3f} sec\".format(end_time - start_time))\n",
        "    print(\"Max memory used by tensors = {} bytes\".format(torch.cuda.max_memory_allocated()))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_model(in_size, out_size, num_layers):\n",
        "    layers = []\n",
        "    for _ in range(num_layers - 1):\n",
        "        layers.append(torch.nn.Linear(in_size, in_size))\n",
        "        layers.append(torch.nn.ReLU())\n",
        "    layers.append(torch.nn.Linear(in_size, out_size))\n",
        "    return torch.nn.Sequential(*tuple(layers)).cuda()"
      ],
      "metadata": {
        "id": "t-852ocXvpQp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 512 # Try, for example, 128, 256, 513.\n",
        "in_size = 4096\n",
        "out_size = 4096\n",
        "num_layers = 3\n",
        "num_batches = 50\n",
        "epochs = 3\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "torch.set_default_device(device)\n",
        "\n",
        "# Creates data in default precision.\n",
        "# The same data is used for both default and mixed precision trials below.\n",
        "# You don't need to manually change inputs' ``dtype`` when enabling mixed precision.\n",
        "data = [torch.randn(batch_size, in_size) for _ in range(num_batches)]\n",
        "targets = [torch.randn(batch_size, out_size) for _ in range(num_batches)]\n",
        "\n",
        "loss_fn = torch.nn.MSELoss().to(device)"
      ],
      "metadata": {
        "id": "ptIRu0Yovva2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = make_model(in_size, out_size, num_layers)\n",
        "opt = torch.optim.SGD(net.parameters(), lr=0.001)\n",
        "\n",
        "start_timer()\n",
        "for epoch in range(epochs):\n",
        "    for input, target in zip(data, targets):\n",
        "        output = net(input)\n",
        "        loss = loss_fn(output, target)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad() # set_to_none=True here can modestly improve performance\n",
        "end_timer_and_print(\"Default precision:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7FOM2h3v5EW",
        "outputId": "2f22134f-3c43-44c2-9ccd-e85b539a6a00"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/cuda/memory.py:491: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Default precision:\n",
            "Total execution time = 5.300 sec\n",
            "Max memory used by tensors = 1284866560 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "use_amp = True\n",
        "\n",
        "net = make_model(in_size, out_size, num_layers)\n",
        "opt = torch.optim.SGD(net.parameters(), lr=0.001)\n",
        "scaler = torch.amp.GradScaler(\"cuda\" ,enabled=use_amp)\n",
        "\n",
        "start_timer()\n",
        "for epoch in range(epochs):\n",
        "    for input, target in zip(data, targets):\n",
        "        with torch.autocast(device_type=device, dtype=torch.float16, enabled=use_amp):\n",
        "            output = net(input)\n",
        "            loss = loss_fn(output, target)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(opt)\n",
        "        scaler.update()\n",
        "        opt.zero_grad() # set_to_none=True here can modestly improve performance\n",
        "end_timer_and_print(\"Mixed precision:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPyReOT6wVf4",
        "outputId": "1de68c71-a34c-4b11-84fc-07c9e5146ff0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Mixed precision:\n",
            "Total execution time = 2.098 sec\n",
            "Max memory used by tensors = 1410720768 bytes\n"
          ]
        }
      ]
    }
  ]
}